# Chapter 1

## Introduction

#### Value of data: 
Fundamental unit of any organization is data. Data derives decision-making in most organizations. e.g.

* Where to locate a new franchise
* What customers to target in marketing
* Where bottlenecks exist in a process
* How customers feel about a product
![Value of Data](https://github.com/Yatish0833/Collecting-storing-and-retrieving-data/blob/master/Images/Big-Data-Value-Continuum-Image-2b.png "Value of Data")
*Fig. 1a Graph of Data value vs Data Age, depicting the importance of individual data and aggregate data.*

Data needs to be in a format that allows for qualitative, quantitative and statistical analysis. In an ideal world data is well organized with no element missing and properly formatted. Although real world data is often unformatted or formatted in a way that is not conducive to analysis or is missing critical pieces. 

![Good data](https://github.com/Yatish0833/Collecting-storing-and-retrieving-data/blob/master/Images/Screen%20Shot%202015-06-26%20at%202.24.38%20PM.png "Good data")

*Fig. 1b Well formatted data*

![Bad data](https://github.com/Yatish0833/Collecting-storing-and-retrieving-data/blob/master/Images/Screen%20Shot%202015-06-26%20at%202.25.09%20PM.png "Bad data")

*Fig. 1c Poorly formatted data with missing values*

#### Role of Data scientist 
Data scientists turn data from various sources into actionable information.
* Collecting the data in the raw form
* Data munging and data wrangling to make it useful for analysis and visualization.
* Cleaning of data to deal with missing values
* Curation of data to make it available for reuse and preservation

## Big Data
Big data is a relatively new term that describes data set that are so large and complex that traditional methods of storing and processing them are not sufficient. The need and importance of big data can be depicted easily with the following statistics:
  
  Every 60 seconds there are 
  * Over 100,000 tweets.
  * 695,000 Facebook status updates.
  * 11 million instant messages.
  * 700,000+ Google searches.
  * 168 million+ emails sent.
  * 1,820 TB of data created.
  * 217 new mobile web users.

Above statistics is just a glimpse of volume of data, which is constantly growing:
* Every day over 2.5 quintillion bytes of data is being generated
* 90% of the world’s data has been generated over the past two years
* Data from multiple sources is being integrated into single massive data sets.

Due to the complexity involved with the term itself there is no single agreed upon definition of “Big Data”, One possible definition:
>Big data is the integration of large amounts of multiple types of structured and unstructured data into a single data set that can be analyzed to gain insight and new understanding of an industry, business, the environment, medicine, disease control, science, and the human interactions and expectations.

##### Examples of Big data:
* The Large Hadron Collider would generate 5 ×1020bytes per day if all of its sensors were turned on, almost 200 times more than all other data sources in the world combined.
* The Square Kilometer Array radio telescope is expected to collect 14 exabytesof data per day for analysis
* Walmart generates over 1 million customer transactions per hour that are curated in a multi-petabyte database for trend analysis

##### Characteristics of Big Data:
* Very large, distributed aggregations of loosely structured data –often incomplete
* In excess of multiple petabytes or exabytesof data
* Billions of records about people or transactions
* Loosely-structured and often distributed data
* Flat schemas with few complex interrelationships
* Time series data containing time-stamped events
* Connections between data elements that must be probabilistically inferred through machine learning

Larger data sets allow for more detailed analysis and application to social sciences, biology, pharmacology, business, marketing and more. Data is everywhere and a lot of it is free. Organizations don't necessarily have to build their own massive data repositories before starting with big data analytics. Steps taken by many companies and government agencies to put large amounts of information into the public domain have made large volumes of data accessible to everyone.

Some of the Important sources of data are:
###### Web Behavior and content:
* There are nearly five billion web pages
* Collected data includes network traffic, site and page visits, page navigation, page searches

###### User Generated Content:
* Also known as "Internet trail" or "Net trail"
* Content generated by millions of users on social media, including Facebook, Twitter, Instagram, blogs, YouTube, forums, wikis, and so forth

###### Activity Generated data:
* Computer and mobile device log files 
* Includes web site tracking information, application logs, sensor data such as check-ins and other location tracking

###### RFID Data:
* Radio Frequency Identifiers
* Tags for tracking merchandise and shipments, mobile payments, sports performance measurement, and automated toll collection

###### Geo Data:
* GPS tracking data generated by mobile devices
* Tracking of movement of equipment, vehicles, and people

###### Environmental Data:
* Weather conditions
* Tidal movements
* Seismic activity

###### Organizational Transactional Data:
* Transactional activities such as purchases, registration, manufacturing

###### Research Data:
* Social science data, e.g., census, polls
* Health care data
* Education, law and order, economic activity, agriculture, food production
* “Big Data” such as radio telescopes, particle physics

Big data is set to offer tremendous insights, but with the terabytes and petabytes of data pouring in to organizations today, traditional architectures are not up to the challenge. There are many challenges which comes along with big data:
###### Analysis - 
With the enormous amount of data available the major challenge is to leverage the value that data have to offer. Big data requires complex analysis within relatively short time spans in order to detect trends and make decisions.
Analysis techniques include, among many others:
  * A/B Testing
  * Visualization
  * Machine Learning
  * Time Series Analysis
  
###### Collection - 
  * Data is not free.
  * Data is in format not conducive to analysis.
  * Data contains missing values or bad entries.
  * Data is not downloadable.

###### Storage - 
Storage of such enormous data is a challenge in itself. There is a need for the system to be able to deal with terabytes/petabytes of data on a daily basis.

###### Curation - 
Curation of data deals with addressing the quality of data. Data has a real value only if it is accurate and timely and thus can help in the decision making process.Poor information quality can be costly:
  * One study estimates that on average bad information costs businesses up to 10% of revenue
  * Another study pegs the loss at over $600 billion annually in the U.S. alone

###### Search and retrieval - 
Timely retrival of meaningful data from the entire data set is one of the most important challenge.

###### Sharing / Transfer - 
Sharing/Transferring data is another concern as there is no platform easily available which allows transfer of such huge data, Organizations tend to invest a lot of money to design special architectures and infrastructures to facilitate data sharing/transfer.

###### Visualization - 
Visualization helps in extracting the meaningful information by processing the data and representing it in a way which can be easily deduced.

###### Privacy - 
Data security becomes the major concern especially when it comes to credit card data, personal ID information or other sensitive assets.

##### Storing Big Data
Traditional data storage technologies including text files, XML, and relational databases reach their limits when used to store very large amounts of data. Furthermore, the data that is needed for analysis includes not only text and numeric data, but unstructured data, such as text files, video, audio, blogs, sensor data, geospatial data, among others. Due to these hurdles storing big data becomes challenging, non relational databases provides a good alternative. Non relational database is a database that does not incorporate the table/key model that relational database management systems (RDBMS) promote. It has the ability to deal with large amount of data and can accomodate unstructured data easily. Fetching data from non relational database provides remarkable speed over relational database as the search query doesn't have to go through each table and key combination in this case.

#### 6 V's of Big Data
##### Volume
Volume is one of the core defining attributes of “big data”.Big Data implies enormous amounts of structured and unstructured data that is generated by social and sensor networks, transaction and search history, and manual data collection. Eg- 100 terabytes of data are uploaded daily to Facebook; Akamai analyses 75 million events a day to target online ads; Walmart handles 1 million customer transactions every single hour.

##### Variety
Data comes from a variety of sources and contains both structured and unstructured data. Data types are not restricted to simply numbers and short text fields, but also include images, emails, text messages, web pages, blog entries, documents, audio, video, and time series. 

##### Velocity
The flow of data that needs to be stored and analyzed is continuous. Human interactions, business processes, machines, and networks generate data continuously and in enormous quantity. The data is generally analyzed in real-time to gain a strategic advantage, which allows companies to do things like display personalised ads on the web pages you visit, based on your recent search, viewing and purchase history. Sampling can help mitigate some of the problems with large data volume and velocity. Eg-
Every minute of every day, we upload 100 hours of video on Youtube, send over 200 million emails and send 300,000 tweets.

##### Veracity
Data veracity characterizes the inherent noise, biases, abnormalities, and mistakes present in virtually all data streams. “Dirty” data presents a significant risk as analyzes are incorrect when based on “bad” data. Data must be cleaned in real-time and processes must be established to keep “dirty data” from accumulating. Data scientist needs to work as a "data janitor" before analysing the data.

##### Validity
While the data may not be “dirty”, biased, or abnormal and it may not be valid for the intended use. Valid data for the intended use is essential to making decisions based on the data.

##### Volatility
Data changes over time and volatility characterizes the degree to which the data changes over time. Decisions and analyses are based on data that has an “expiration date”. Data scientists must define at what point in time a data stream is no longer relevant and cannot be used to make a decision.

#### Additional V's
##### Viscosity
Viscosity measures the resistance to flow in the volume of data, resistance to navigate in the dataset, data flow rates or complexity of data processing required. Technologies to deal with viscosity include improved streaming, agile integration bus, and complex event processing.
##### Virality
Virality measures how quickly data is spread and shared to each unique node. Time is an important characteristic along with rate of proliferation. Virality of data can provide companies with instant insights into the target areas to launch marketing campaigns.

> #### Checkpoint
Concepts Pharma has built a data repository that collects self-reported eating habits of clinical trial participants through a mobile habit. The translation medicine group is using the data to determine if the drug in trial is causing digestive issues when taken with certain food groups. Which of the V’s should be of most concern to them?

> 1. Veracity
2. Volume
3. Volatility
4. Velocity
5. Variety

> _Answer at the end of chapter_

### Planning A Big Data project
Planning a big data project is a complicated task which requires expertise in many different areas, data scientist or data analyst needs to be well versed with the following concepts:
* Objectives
* Data
* Process
* Infrastructure
* Analytics

##### Objectives
Objectives needs to be clearly defined with proper outline of every step of the project. Data scientist needs to answer the questions like:
* What is the purpose of the data project?
* How is the data going to be used?
* What is the business or organizational value of the data project?

##### Data
* What data needs to be collected?
* Where will the data come from?
  * Internal systems?
  * Social networks?
  * External data sources?
* What is the structure of the data?
  * Quantitative or qualitative?
* What is the quality of the data?


##### Processes
* How will it be collected?
* Who is involved in collection of the data?
* How will the data be cleaned?
* How will the data be loaded and transferred?
* What kind of analysis needs to be done?
  * Real time analysis?

##### Infrastructure
* Where will the data be stored?
* What database or data store will be needed based on the volume, complexity, type, and required access of the data?
* What hardware is needed to support responsive access to the data?
* Who will manage the data store?
* Who will supply the data store?

##### Analytics
* How will the data be presented?
  * Tables?
  * Visualizations?
* What predictive models will be built?
* How will the data from different sources be combined?
* What skills are needed to do the analysis?
* What programs or applications need to be built or purchased?
